---
title: "深度、规模与突破：AI 十五年巨变与 Gemini 的诞生之路"
date: "2025-11-27"
description: "Jeff Dean 深度复盘 Google AI 十五年的演进历程：从 Disbelief 系统到 Transformer 架构，再到 Gemini 的多模态突破，揭示算力、算法与规模化如何重塑人工智能。"
category: "Artificial Intelligence"
tags: ["Google DeepMind", "Jeff Dean", "Gemini", "Transformer", "TPU", "Large Language Models"]
---

# 深度、规模与突破：AI 十五年巨变与 Gemini 的诞生之路

## 引言：AI 如何彻底改变我们的预期

在过去的十年左右，**机器学习** 彻底改变了 我们对计算机能力的预期。十年前，计算机在自然语音识别、图像识别或理解语言方面表现并不出色。如今，我们正处于一个由大规模、高性能模型驱动的智能时代。

这场革命的背后，是无数基础性创新和工程积累。今天的分享将回顾促成当今先进模型的 **一系列重要进展**。

首先，我们来认识一下这场变革的关键人物：**Jeff Dean**。他于1999年加入Google，是Google的第30位员工。他构建了包括 MapReduce、BigTable 和 Spanner 在内的奠基性基础设施。2011年，他创立了 Google Brain，并发布了世界最受欢迎的深度学习框架之一 TensorFlow。现在，Jeff Dean担任 Google DeepMind 和 Google Research 的首席科学家，并领导着 Gemini 团队。

他指出，核心范式是 **深度学习方法（神经网络）和规模的增加** 带来了显著成果。同时，计算的类型和运行计算的 **硬件** 也发生了巨大变化。

## 第一阶段：规模化训练的起点（2012）

神经网络和反向传播（Back Propagation）是相对古老的想法。Jeff Dean早在1990年就对神经元并行训练进行了研究，但他承认当时 **严重低估了所需的处理能力**，错误地认为需要32倍，而实际需要 **一百万倍**。

直到 **2012年 Google Brain 项目** 的诞生，才将规模化训练提上日程。

*   **“Disbelief”系统：** 这是Google Brain早期构建的软件抽象，支持模型并行和数据并行。在2012年，该系统使团队能够训练比之前任何人训练的神经网络 **大50到100倍** 的模型。
*   **“猫论文”与无监督学习：** 利用Disbelief，团队通过无监督目标函数（重建原始像素）训练了从1000万个YouTube视频帧中提取的数据。结果，模型中出现了对高级概念（如猫、人脸）敏感的神经元，即使从未被告知“猫”是什么。这种无监督预训练将ImageNet 22,000类别基准的最新技术水平 **相对提高了70%**。
*   **语言处理基础：** 随后发展了 **分布式词表示（Word Embeddings）**，用高维向量表示词汇。这些向量空间中的方向被证明具有意义（例如，通过向量减法来改变词的性别或动词时态）。
*   **序列到序列（Seq2Seq）：** 使用LSTMs将一个序列映射到另一个序列，最初应用于 **高质量的翻译任务**。

## 第二阶段：专用硬件的革命

随着神经网络的成功应用（如高质量语音识别），Jeff Dean意识到，若要全面推广，所需的计算资源将需要Google **将计算机数量增加一倍**。解决方案是专用硬件。

*   **TPU的设计洞察：** 神经网络计算容忍 **低精度** 计算，且主要由密集的线性代数运算（矩阵乘法）组成。因此，可以构建专门用于加速降精度线性代数的硬件。
*   **TPUv1 (2015)：** 专注于加速 **推理**，比当时的CPU和GPU快 **15到30倍**，能效高 **30到80倍**。该论文成为ISA 50年历史上被引用次数最多的论文。
*   **机器学习超级计算机：** 后续的TPU设计系列专注于 **训练** 需求，集成了低精度计算、高速定制网络和高级编译器。例如，最新的 Ironwood Pod 拥有9,216个芯片，通过3D环面连接，其峰值性能是第一代ML超级计算机TPUv2的 **约3600倍**。

## 第三阶段：Transformer 架构与高效计算

硬件性能的飞跃伴随着算法和架构的突破。

*   **TensorFlow与开源生态：** Google开源了 TensorFlow（Disbelief的继任者）。社区中，PyTorch（Python版）和Jax（功能性计算）也极大地推动了机器学习发展。
*   **Transformer (2017)：** 引入了基于 **注意力（attention-based）** 的机制，成为现代语言模型的核心。与传统模型不同，Transformer保存所有状态，并允许模型在需要时关注（attend to）它们。这带来了更高的准确性，同时所需的计算量减少了 **10到100倍**。
*   **大规模语言模型：** 利用海量文本进行 **自监督学习**，无论是通过自回归（预测下一个词）还是掩码语言建模（预测缺失词）。
*   **视觉Transformer：** 将Transformer应用于计算机视觉，实现了最佳结果，同时所需的计算量减少了 **4到20倍**。
*   **稀疏模型（Sparse Models）：** 这是一个关键的效率优化。其理念是只激活一个超大型模型中 **1%到5%** 的参数来进行预测，从而实现了训练成本（计算量）约 **8倍** 的降低。Jeff Dean强调，如今大多数模型（如Gemini）都采用了稀疏模型。
*   **Pathways：** 为了支持大规模、复杂的稀疏模型训练，Pathways系统被构建出来，旨在简化和调度计算。它能够协调跨Pod、跨建筑甚至跨多个大都会区域的计算和网络通信。它可以让一个Jax编程环境看起来像拥有 **10,000个设备**。

## 第四阶段：模型优化与智能飞跃

除了基础架构，Jeff Dean也介绍了提高模型推理能力和效率的优化技术。

*   **思维链提示（Chain-of-Thought Prompting）：** 通过提示模型 **展示其工作流程（逐步推理）**，可以显著提高模型在复杂问题（如初中数学基准GSM8K）上的准确性。
*   **蒸馏（Distillation）：** 使用一个非常好的大型“教师”模型来提供 **软目标**（对缺失词汇的概率分布）给较小的“学生”模型。Jeff Dean参与的这项技术表明，即使学生模型只使用 **3%的训练数据**，也能获得接近教师模型的性能，是构建高质量小型模型的关键。
*   **后训练强化学习（RL）：** 在模型完成自监督训练后，用于鼓励正确的行为（如礼貌、安全属性）和增强能力。尤其在 **可验证域**（如数学证明或编程代码）中，可以通过运行证明检查器或编译单元测试来提供精确的奖励信号，极大提升了模型探索和解决复杂问题的能力。

## Gemini：集成创新的集大成者

Jeff Dean强调，Google正在开发的 **Gemini模型** 结合了上述许多思想，其目标是训练 **世界上最好的多模态模型**。

*   **多模态设计：** Gemini从一开始就被设计为多模态，能够接收多种输入（如文本、图像）并生成多种输出（如视频、音频）。
*   **数学推理能力的飞跃：** 模型能力进步惊人。2022年模型只能解决简单的算术问题。而 **Gemini 2.5 Pro** 的一个变体，在今年的 **国际数学奥林匹克竞赛（IMO）** 中解决了六个问题中的五个，达到了 **金牌水平**。
*   **最新成果：** 最近发布的 **Gemini 3.0 Pro** 在各项基准测试中表现出色，并在非基准测试的LM Arena排名中位居第一。模型在 **Web开发风格的编程** 方面有了巨大的飞跃。
*   **应用演示：** Gemini的能力包括根据高层指令生成代码和动画，以及结合转录、翻译和图像生成能力，从扫描的多种语言食谱创建双语网站。它甚至能展示 **中间图像推理** 能力（例如逐步分析球的运动路径），并能根据用户指示修改信息图（如添加冥王星和幽默评论）。

## 结论与展望：光明的未来与潜在的挑战

Jeff Dean总结道，这些模型正变得 **极其强大**。持续的研究和创新将继续这一趋势，对包括 **医疗保健、教育、科学研究和媒体创作** 在内的多个领域产生巨大影响。

AI的进步可能使深层次的专业知识能够被更多人利用。例如，通过辅助编码，即使是未受过专业编程训练的人也能生成复杂的网站。

然而，Jeff Dean也指出，我们并非完全忽略潜在风险。例如，**虚假信息（misinformation）** 是一个值得关注的领域。他和同事们正在研究如何确保获得医疗保健和教育等方面的巨大益处，同时将虚假信息等潜在负面影响 **降至最低**。

这场由规模、硬件和架构创新共同推动的AI变革，正在引领我们走向一个由AI辅助的、充满希望的未来。

---

> **引用来源**：本文内容整理自 Google DeepMind 首席科学家 Jeff Dean 关于人工智能发展历史与 Gemini 模型技术演进的主题分享。[Stanford AI Club: Jeff Dean 谈重要的人工智能趋势](https://www.youtube.com/watch?v=AnTw_t21ayE)